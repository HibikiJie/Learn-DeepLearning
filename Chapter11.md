# Chapter11

# 模型压缩

- 深度学习（Deep Learning）因其计算复杂度或参数冗余，在一些场景和设备上限制了相应的模型部署，需要借助模型压缩、优化加速、异构计算灯方法突破瓶颈。
- 模型压缩算法能够有效降低参数冗余，从而减少存储占用、通行带宽和计算复杂度，有助深度学习的应用部署，具体可划分为如下几种方法：
  1. 线性或非线性化：1/2bits，int8和fp16等；
  2. 结构或非结构剪枝：deep compression，channel pruning和network slimming等；
  3. 知识蒸馏与网络结构简化（squeeze-net，mobile-net，shffle-net）等；





# 1、剪枝

![image-20201021102727850](D:%5CLearn-DeepLearning%5Cimage%5Cimage-20201021102727850.png)

![image-20201021105411830](D:%5CLearn-DeepLearning%5Cimage%5Cimage-20201021105411830.png)

其基本的思想是：**神经网络的参数众多，但其中有些参数对最终的输出结果贡献不大而显得冗余，剪枝顾名思义，就是要将这些冗余的参数剪掉。**

剪枝有减去权重的非结构化的剪枝方式和减去神经元的结构化的剪枝方式。卷积则是剪去参数或是卷积核。

剪枝方式：

- 非结构剪枝：通常是连接级、细粒度的剪枝方式，精度相对较高，但依赖于特定算法库或硬件平台支持。
- 结构剪枝：是filter级（卷积核）或是layer级（神经元）、粗粒度的剪枝方法，精度相对较低，但剪枝侧率更为有效，不需要特定的算法库或硬件平台的支持，能够直接在成熟深度学习框架上运行。
  1. 局部方式的、通过layer by layer方式的、最小化输出FM重建误差的Channel pruning，ThiNet Discrimination-aware Channel Pruning;
  2. 全局方式的、通过训练期间对BN称Gamma系数施加L1正则约束的网络压缩（Network Slimming）





![image-20201021103914269](D:%5CLearn-DeepLearning%5Cimage%5Cimage-20201021103914269.png)

剪枝后，将新的参数赋予模型，再训练一会儿。再剪、再训练、……。

既然要修剪，为什么不直接先设计一个小网络？

网络越大、维度越高，对数据抽象化能力越强，网络泛化能力是足够的。但实际应用的时候，大量的权重对解决当前问题时，贡献力度是不同的。有很多的权重对最终结果的影响是微乎其微的。但剪枝过狠，将对贡献度大的剪去，精度损失则会增加

![image-20201021100509556](D:%5CLearn-DeepLearning%5Cimage%5Cimage-20201021100509556.png)

模型比较大的时候，在模型压缩后，精度会有损失，但是不多。

如何判断贡献度呢？

网络权重在-1~1之间，接近于零，但权重会大量为零吗？

L1正则化会将参数压制至零，L2只会压缩至很小的范围。

于是在训练网络时，就进行L1正则化，然后剪枝参数。



还有随机减去神经元、卷积核。Retraining时会加入新的数据集训练，并且是一点一点的剪。

卷积有随机剪权重、向量等级的剪枝、核级别的剪枝、组上的剪枝，和卷积核的卷积（直接剪去卷积核）。

**一般都是采用剪去神经元或卷积核。**

且都是一层一层的局部剪枝，而非整个网络全局剪枝（可能直接剪去一层）。



# 2、量化

- 低精度（Low Precision）可能是最通用的概念。常规精度一般使用FP32（32位浮点，单精度）存储模型权重；低精度则表示FP16（半精度浮点），int8（8位的定点整数）等等数值格式。不过目前低精度往往指代int8.
- 混合精度（Minxed Precision）在模型中使用FP32和FP16。在FP16减少了一半的内存大小，但有些参数或操作符必须采用FP32格式才能保持准确度。
- 量化一般指代INT8
- 根据存储一个权重元素所需的位数，还可以包括：
  1. 二值神经网络：在运行时权重和激活只取两种值（-1，+1）的神经网络，以及在训练时计算参数的梯度。
  2. 三元权重网络：权重约束为+1、0、-1的神经网络。
  3. XNOR网络：过滤器和卷积层的输入是二进制的。网络主要使用二进制运算来近似卷积。



## 量化原理

![image-20201021131525027](D:%5CLearn-DeepLearning%5Cimage%5Cimage-20201021131525027.png)

将一个范围的数据，映射到另一个有限范围内，为**量化**

模型量化，这是在模型运行过程中，转变为低精度运算，然后再反量化回正常数值范围。







# 3、蒸馏

“蒸馏”（**distillation**）：把大网络的知识压缩成小网络的一种方法

**具体做法（How）**

1. **蒸馏**：先训练好一个大网络，在最后的softmax层使用合适的温度参数T，最后训练得到的概率称为“软目标”。以这个软目标和真实标签作为目标，去训练一个比较小的网络，训练的时候也使用在大模型中确定的温度参数T
2. **专用模型**：对于一个已经训练好的大网络，可以训练一系列的专用模型，每个专用模型只训练一部分专用的类以及一个“不属于这些专用类的其它类”，比如专用模型1训练的类包括“显示器”，“鼠标”，“键盘”，...，“其它”；专用模型2训练的类包括“玻璃杯”，“保温杯”，“塑料杯”，“其它“。最后以专用模型和大网络的预测输出作为目标，训练一个最终的网络来拟合这个目标。



**意义（Why）**

1. 蒸馏**把大网络压成小网络**，这样就可以先在训练阶段花费大精力训练一个大网络，然后**在部署阶段以较小的计算代价来产生一个较小的网络**，同时保持一定的网络预测表现。
2. 对于一个已经训练好的大网络，如果要去做集成的话计算开销是很大的，可以在这个基础上**训练一系列专用模型**，因为这些模型通常比较小，所以训练会快很多，而且有了这些专用模型的输出可以得到一个软目标，实验证明使用软目标训练可以减小过拟合。最后根据这个大网络和一系列专用模型的输出作为目标，训练一个最终的网络，可以得到不错的表现，而且**不需要对大网络做大量的集成计算**。





softmax公式： 
$$
q_{i}=\frac{e^{(z_{i}/T)}}{∑_{j}e^{(z_{j}/T)}}
$$
其中温度参数T通常设置为1，T越大可以得到更“软”的概率分布。
（**T越大，不同激活值的概率差异越小，所有激活值的概率趋于相同；T越小，不同激活值的概率差异越大**）
（**在蒸馏训练的时候使用较大的T的原因是，较小的T对于那些远小于平均激活值的单元会给予更少的关注，而这些单元是有用的，使用较高的T能够捕捉这些信息**）

最简单的蒸馏形式就是，训练小模型的时候，以复杂模型得到的“软目标”为目标，采用复杂模型中的较高的T，训练完之后把T改为1。

当部分或全部转移训练集的正确标签已知时，蒸馏得到的模型会更优。一个方法就是使用正确标签来修改软目标。

但是我们发现一个更好的方法，简单对两个不同的目标函数进行权重平均，第一个目标函数是和复杂模型的软目标做一个交叉熵，使用的复杂模型的温度T；第二个目标函数是和正确标签的交叉熵，温度设置为1。我们发现第二个目标函数被分配一个低权重时通常会取得最好的结果。

因此：

1. 首先用较大的T值来训练模型，这时候复杂的神经网络能够产生更均匀分布的软目标；
2. 之后小规模的神经网络用**相同的T值**来学习由大规模神经产生的软目标，接近这个软目标从而学习到数据的结构分布特征；
3. 最后在实际应用中，将T值恢复到1，让类别概率偏向正确类别。